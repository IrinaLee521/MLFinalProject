{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The nPrint Machine Learning Pipeline\n",
    "\n",
    "This notebook is designed to give you a **very** simple example of how to use nPrint in a generic machine learning pipeline and the rapid pace at which we can train new models and test new ideas on network traffic. Note that this example is simply to show the pipeline, not to test a hard problem. The traffic collected is to the same website over the course of about 15 seconds.\n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "nPrint must be installed into $PATH for external commands to work\n",
    "\n",
    "### Directory Sturcture\n",
    "\n",
    "There are 2 `pcap` files in this directory\n",
    "1. `port443.pcap` - a small trace of packets sent and received over https  \n",
    "2. `port80.pcap` - a small trace of packets sent and received over http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating nPrints from Traffic\n",
    "\n",
    "First, lets generate nPrints from each traffic trace. Let's **only** include the TCP headers in the nPrints for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets examine the nPrints, which can be directly loaded with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "friday = pd.read_csv('TrafficLabelling/Friday-WorkingHours-Morning.pcap_ISCX.csv.gz', index_col=0)\n",
    "\n",
    "print('Friday nPrint: Number of Packets: {0}, Features per packet: {1}'.format(friday.shape[0], friday.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nPrint to Machine Learning Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We label each sample with its corresponding operating system by matching its source IP address to the OS found\n",
    "on this website https://www.unb.ca/cic/datasets/ids-2017.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math \n",
    "samples = []\n",
    "labels = []\n",
    "\n",
    "friday = friday.replace([np.inf, -np.inf], np.nan)\n",
    "friday = friday.dropna()\n",
    "# label the operating system based on IP address from this website: https://www.unb.ca/cic/datasets/ids-2017.html\n",
    "ip_to_os = {'205.174.165.73': 'Kali', '205.174.165.69': 'Win', '205.174.165.70': 'Win', '205.174.165.71': 'Win', '192.168.10.50': 'Web server 16 Public', '192.168.10.205.174.165.68': 'Web server 16 Public', '192.168.10.51': 'Ubuntu server 12 Public', '192.168.10.205.174.165.66': 'Ubuntu server 12 Public', '192.168.10.19': 'Ubuntu 14.4, 32B', '192.168.10.17': 'Ubuntu 14.4, 64B', '192.168.10.16': 'Ubuntu 16.4, 32B', '192.168.10.12': 'Ubuntu 16.4, 64B', '192.168.10.9': 'Win 7 Pro, 64B', '192.168.10.5': 'Win 8.1, 64B', '192.168.10.8': 'Win Vista, 64B', '192.168.10.14': 'Win 10, pro 32B', '192.168.10.15': 'Win 10, 64B', '192.168.10.25': 'MAC'}\n",
    "labels = friday[' Source IP'].apply(lambda x : \"Other\" if x not in ip_to_os else ip_to_os[x])\n",
    "samples = friday.drop([' Source IP', ' Source Port', ' Destination IP', ' Destination Port', ' Timestamp'], axis=1)\n",
    "samples = samples._get_numeric_data()\n",
    "samples = samples.reset_index(drop=True)\n",
    "\n",
    "samples = samples.astype(np.float32)\n",
    "samples.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Classifier\n",
    "\n",
    "We're already ready to train and test a model on the traffic we gathered. Let's split the data into training and testing data, train a model, and get a stat report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels)\n",
    "\n",
    "# Initialize Classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0)\n",
    "\n",
    "# Train \n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Statistics\n",
    "\n",
    "# First, lets get a stat report about the precision and recall:\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Let's also get the ROC AUC score while we're here, which requires a probability instead of just the prediction\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "# predict_proba gives us a probability estimate of each class, while roc_auc just cares about the \"positive\" class\n",
    "y_pred_proba_pos = [sublist[1] for sublist in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovo\")\n",
    "print('ROC AUC Score: {0}'.format(roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTrees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels)\n",
    "\n",
    "# Initialize Classifier\n",
    "clf = ExtraTreesClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0)\n",
    "\n",
    "# Train \n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Statistics\n",
    "\n",
    "# First, lets get a stat report about the precision and recall:\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Let's also get the ROC AUC score while we're here, which requires a probability instead of just the prediction\n",
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "# predict_proba gives us a probability estimate of each class, while roc_auc just cares about the \"positive\" class\n",
    "y_pred_proba_pos = [sublist[1] for sublist in y_pred_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = roc_auc_score(y_test, y_pred_proba, multi_class=\"ovo\")\n",
    "print('ROC AUC Score: {0}'.format(roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the model\n",
    "\n",
    "nPrint's alignment of each packet allows for understanding the specific features (parts of the packet) that are driving the model's performance. It turns out that the options that are being set in the TCP header is actually more important than the port numbers themselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Raw feature importances\n",
    "feature_importances = clf.feature_importances_\n",
    "# Match the feature names we know with the importances\n",
    "named_importances = []\n",
    "for column_name, importance in zip(nprint_80.columns, feature_importances):\n",
    "    named_importances.append((column_name, importance))\n",
    "# Sort the named feature importances\n",
    "sorted_feature_importances = sorted(named_importances, key=lambda tup: tup[1], reverse=True)\n",
    "# Now lets print the top 20 important features (bits)\n",
    "print(*sorted_feature_importances[0:20], sep='\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapidly testing different versions of the problem\n",
    "\n",
    "now that we have a generic pipeline, we can leverage nPrint's flags to generate different versions of nPrints. Let's test a version of this classification problem using **only** the IPv4 Headers of the packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nPrints\n",
    "cmd_80 = 'nprint -P port80.pcap -4  -W port80.npt'\n",
    "cmd_443 = 'nprint -P port443.pcap -4 -W port443.npt'\n",
    "!{cmd_80}\n",
    "!{cmd_443}\n",
    "\n",
    "# Load nPrints\n",
    "nprint_80 = pd.read_csv('port80.npt', index_col=0)\n",
    "nprint_443 = pd.read_csv('port443.npt', index_col=0)\n",
    "\n",
    "# Assoicate with Labels\n",
    "samples = []\n",
    "labels = []\n",
    "for _, row in nprint_80.iterrows():\n",
    "    samples.append(np.array(row))\n",
    "    labels.append('unencrypted')\n",
    "\n",
    "for _, row in nprint_443.iterrows():\n",
    "    samples.append(np.array(row))\n",
    "    labels.append('encrypted')\n",
    "    \n",
    "# Train and Test the Classifier\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels)\n",
    "# Initialize Classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0)\n",
    "# Train \n",
    "clf.fit(X_train, y_train) \n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "# Statistics\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about Testing using just the first 30 payload bytes in each packet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate nPrints\n",
    "cmd_80 = 'nprint -P port80.pcap -p 30 -W port80.npt'\n",
    "cmd_443 = 'nprint -P port443.pcap -p 30 -W port443.npt'\n",
    "!{cmd_80}\n",
    "!{cmd_443}\n",
    "\n",
    "# Load nPrints\n",
    "nprint_80 = pd.read_csv('port80.npt', index_col=0)\n",
    "nprint_443 = pd.read_csv('port443.npt', index_col=0)\n",
    "\n",
    "# Assoicate with Labels\n",
    "samples = []\n",
    "labels = []\n",
    "for _, row in nprint_80.iterrows():\n",
    "    samples.append(np.array(row))\n",
    "    labels.append('unencrypted')\n",
    "\n",
    "for _, row in nprint_443.iterrows():\n",
    "    samples.append(np.array(row))\n",
    "    labels.append('encrypted')\n",
    "    \n",
    "# Train and Test the Classifier\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels)\n",
    "# Initialize Classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0)\n",
    "# Train \n",
    "clf.fit(X_train, y_train) \n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "# Statistics\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much harder problem, with a much lower score. It may be likely that many packets don't have a payload at all, making it impossible to guess the traffic! What if we remove those packets from our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   encrypted       0.67      1.00      0.80       440\n",
      " unencrypted       1.00      0.24      0.39       284\n",
      "\n",
      "    accuracy                           0.70       724\n",
      "   macro avg       0.84      0.62      0.60       724\n",
      "weighted avg       0.80      0.70      0.64       724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load nPrints\n",
    "nprint_80 = pd.read_csv('port80.npt', index_col=0)\n",
    "nprint_443 = pd.read_csv('port443.npt', index_col=0)\n",
    "\n",
    "# Assoicate with Labels\n",
    "samples = []\n",
    "labels = []\n",
    "for _, row in nprint_80.iterrows():\n",
    "    # Check for no payload, all bits will be -1. There are more efficient ways to do this\n",
    "    if len(set(row)) == 1:\n",
    "        continue\n",
    "    samples.append(np.array(row))\n",
    "    labels.append('unencrypted')\n",
    "\n",
    "for _, row in nprint_443.iterrows():\n",
    "    # Check for no payload, all bits will be -1. There are more efficient ways to do this\n",
    "    if len(set(row)) == 1:\n",
    "        continue\n",
    "    samples.append(np.array(row))\n",
    "    labels.append('encrypted')\n",
    "    \n",
    "# Train and Test the Classifier\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(samples, labels)\n",
    "# Initialize Classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0)\n",
    "# Train \n",
    "clf.fit(X_train, y_train) \n",
    "# Predict\n",
    "y_pred = clf.predict(X_test)\n",
    "# Statistics\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hopefully this gives you a better idea of how nPrint can be used to rapidly train and test models for different traffic analysis problems. While this problem was contrived and simple, the same basic steps can be performed for any single-packet classification problem. If you want to train and test using **sets** of packets as input to a model, you'll either need a model that can handle that input, such as a CNN, or to flatten the 2D packet sample into a 1d sample for use with a model such as the random forest above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
